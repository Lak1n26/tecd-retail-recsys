# Training configuration
epochs: 10
learning_rate: 0.001
weight_decay: 0.0001
optimizer: adam

# Learning rate scheduler
scheduler:
  name: cosine
  warmup_epochs: 1
  min_lr: 0.00001

# Early stopping
early_stopping:
  patience: 3
  monitor: val_ndcg_100
  mode: max

# Gradient clipping
gradient_clip_val: 1.0

# Checkpointing
save_top_k: 3
checkpoint_monitor: val_ndcg_100

# Evaluation
eval_k: [10, 50, 100]

# PyTorch Lightning Trainer
trainer:
  accelerator: auto
  devices: 1
  precision: 32
  log_every_n_steps: 50
  val_check_interval: 1.0
